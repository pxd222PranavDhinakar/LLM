{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class ArithmeticDataset(Dataset):\n",
    "    def __init__(self, max_length, num_samples):\n",
    "        self.max_length = max_length\n",
    "        self.num_samples = num_samples\n",
    "        self.data = self.generate_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def generate_number(self, length):\n",
    "        return random.randint(10**(length-1), 10**length - 1)\n",
    "\n",
    "    def generate_data(self):\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditionDataset(ArithmeticDataset):\n",
    "    def generate_data(self):\n",
    "        data = []\n",
    "        samples_per_combination = self.num_samples // (self.max_length ** 2)\n",
    "        for i in range(1, self.max_length + 1):\n",
    "            for j in range(1, self.max_length + 1):\n",
    "                for _ in range(samples_per_combination):\n",
    "                    num1 = self.generate_number(i)\n",
    "                    num2 = self.generate_number(j)\n",
    "                    result = num1 + num2\n",
    "                    data.append((f\"{num1}+{num2}=\", str(result)))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiplicationDataset(ArithmeticDataset):\n",
    "    def generate_data(self):\n",
    "        data = []\n",
    "        samples_per_combination = self.num_samples // (self.max_length ** 2)\n",
    "        for i in range(1, self.max_length + 1):\n",
    "            for j in range(1, self.max_length + 1):\n",
    "                for _ in range(samples_per_combination):\n",
    "                    num1 = self.generate_number(i)\n",
    "                    num2 = self.generate_number(j)\n",
    "                    result = num1 * num2\n",
    "                    data.append((f\"{num1}*{num2}=\", str(result)))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortingDataset(ArithmeticDataset):\n",
    "    def generate_data(self):\n",
    "        data = []\n",
    "        samples_per_combination = self.num_samples // (self.max_length ** 2)\n",
    "        for i in range(1, self.max_length + 1):  # number of integers\n",
    "            for j in range(1, self.max_length + 1):  # max digit length\n",
    "                for _ in range(samples_per_combination):\n",
    "                    numbers = [self.generate_number(random.randint(1, j)) for _ in range(i)]\n",
    "                    indices = list('abcdefghijklmnopqrstuvwxyz'[:i])\n",
    "                    input_str = ','.join([f\"{idx}:{num}\" for idx, num in zip(indices, numbers)])\n",
    "                    sorted_indices = [idx for _, idx in sorted(zip(numbers, indices))]\n",
    "                    output_str = ''.join(sorted_indices)\n",
    "                    data.append((input_str, output_str))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(dataset_class, max_length, train_samples, test_samples):\n",
    "    train_dataset = dataset_class(max_length, train_samples)\n",
    "    test_dataset = dataset_class(max_length, test_samples)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition sample: ('4+5=', '9')\n",
      "Multiplication sample: ('6*4=', '24')\n",
      "Sorting sample: ('a:3', 'a')\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "max_length = 20  # maximum length of operands\n",
    "train_samples = 200_000  # 20 million as mentioned in the paper\n",
    "test_samples = 1_000  # adjust as needed\n",
    "\n",
    "# Create datasets\n",
    "addition_train, addition_test = create_datasets(AdditionDataset, max_length, train_samples, test_samples)\n",
    "multiplication_train, multiplication_test = create_datasets(MultiplicationDataset, max_length, train_samples, test_samples)\n",
    "sorting_train, sorting_test = create_datasets(SortingDataset, max_length, train_samples, test_samples)\n",
    "\n",
    "# Print some samples\n",
    "print(\"Addition sample:\", addition_train[0])\n",
    "print(\"Multiplication sample:\", multiplication_train[0])\n",
    "print(\"Sorting sample:\", sorting_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Addition Samples:\n",
      "Input: 135748326+108631275=, Output: 244379601\n",
      "Input: 1354172+336685848355199866=, Output: 336685848356554038\n",
      "Input: 44338320546283+972243690165=, Output: 45310564236448\n",
      "Input: 75040161704634192095+9527451304538227395=, Output: 84567613009172419490\n",
      "Input: 9318+84623911195379659736=, Output: 84623911195379669054\n",
      "Input: 58739854+8279644088=, Output: 8338383942\n",
      "Input: 9395564888+77629=, Output: 9395642517\n",
      "Input: 520+2371=, Output: 2891\n",
      "Input: 5623061952901691460+3=, Output: 5623061952901691463\n",
      "Input: 243+4925787039520747=, Output: 4925787039520990\n",
      "\n",
      "Multiplication Samples:\n",
      "Input: 1314*4727797=, Output: 6212325258\n",
      "Input: 3228169008960561*26=, Output: 83932394232974586\n",
      "Input: 7035962375386837078*20466463929=, Output: 144001270161655858478286759462\n",
      "Input: 4830844194151411*4972082029=, Output: 24019353602639217538092919\n",
      "Input: 68464*476769075851088=, Output: 32641518009068888832\n",
      "Input: 3517314548080498*75912257251479930246=, Output: 267007286808259638068510650232942508\n",
      "Input: 9157701523120469911*24=, Output: 219784836554891277864\n",
      "Input: 9815449*39=, Output: 382802511\n",
      "Input: 222564*1319102320=, Output: 293584688748480\n",
      "Input: 5103973233470414*7566984585775=, Output: 38621686783878808047507760850\n",
      "\n",
      "Sorting Samples:\n",
      "Input: a:89850,b:1,c:210273, Output: bac\n",
      "Input: a:92422980069,b:955,c:166596852822693,d:33168598266,e:9643845828,f:7128107932,g:240138997812,h:861004747392283,i:5,j:649348680291496,k:899641678955276,l:210912946985052,m:7,n:3946970,o:202021969566434, Output: imbnfedagcoljhk\n",
      "Input: a:4,b:56203099, Output: ab\n",
      "Input: a:11,b:88,c:2,d:87,e:3,f:4,g:805,h:844,i:31,j:121,k:91,l:819,m:8,n:4,o:75, Output: cefnmaiodbkjglh\n",
      "Input: a:11, Output: a\n",
      "Input: a:9,b:9, Output: ab\n",
      "Input: a:25,b:2,c:27492444666,d:553,e:74497453366,f:248566,g:1697,h:50303842,i:341,j:90,k:913039,l:64421985863,m:3262,n:7,o:26425471,p:75906992651,q:973,r:24251515,s:2172,t:25242, Output: bnajidqgsmtfkrohclep\n",
      "Input: a:5,b:4, Output: ba\n",
      "Input: a:9614,b:413619,c:2103866,d:525,e:35,f:497705,g:805690,h:34135,i:66437,j:5,k:56498,l:48097670,m:9982,n:626,o:15513,p:47, Output: jepdnamohkibfgcl\n",
      "Input: a:821,b:51872,c:9,d:72,e:10414,f:98082,g:41,h:984,i:30,j:2,k:9898,l:13, Output: jcligdahkebf\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def print_samples(dataset, name, num_samples=10):\n",
    "    print(f\"\\n{name} Samples:\")\n",
    "    for _ in range(num_samples):\n",
    "        idx = random.randint(0, len(dataset) - 1)\n",
    "        sample = dataset[idx]\n",
    "        print(f\"Input: {sample[0]}, Output: {sample[1]}\")\n",
    "\n",
    "# Sample from Addition dataset\n",
    "print_samples(addition_train, \"Addition\")\n",
    "\n",
    "# Sample from Multiplication dataset\n",
    "print_samples(multiplication_train, \"Multiplication\")\n",
    "\n",
    "# Sample from Sorting dataset\n",
    "print_samples(sorting_train, \"Sorting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Transformer for Arithmetic Tasks\n",
    "\n",
    "This code implements a small transformer model designed to learn basic arithmetic operations, inspired by the Abacus Embeddings paper. The model architecture is as follows:\n",
    "\n",
    "## Model Architecture\n",
    "- Embedding layer: Custom Abacus Embedding\n",
    "- Transformer layers: 2\n",
    "- Attention heads per layer: 2\n",
    "- Embedding dimension: 64\n",
    "- Feed-forward dimension: 128\n",
    "- Maximum sequence length: 20\n",
    "\n",
    "## Key Components\n",
    "1. **AbacusEmbedding**: A custom embedding layer that combines token embeddings with positional information.\n",
    "2. **SmallTransformer**: The main model class, incorporating the Abacus Embedding and transformer layers.\n",
    "3. **Training Loop**: Includes both training and evaluation phases, tracking loss and accuracy.\n",
    "\n",
    "## Training Details\n",
    "- Dataset: Addition task (can be extended to multiplication and sorting)\n",
    "- Batch size: 32\n",
    "- Number of epochs: 10\n",
    "- Optimizer: Adam\n",
    "- Learning rate: 0.001\n",
    "- Loss function: Cross Entropy Loss (ignoring padding tokens)\n",
    "\n",
    "This setup allows for quick experimentation and debugging on a CPU. Once the basic functionality is verified, the model size and dataset can be scaled up to match the specifications in the Abacus Embeddings paper.\n",
    "\n",
    "Let's calculate the number of parameters for this model configuration. We'll break it down by component:\n",
    "\n",
    "1. Embedding Layer:\n",
    "   - Token Embedding: vocab_size * embed_size = 14 * 64 = 896\n",
    "   - Positional Embedding: max_length * embed_size = 20 * 64 = 1,280\n",
    "\n",
    "2. Transformer Layers (for each layer):\n",
    "   - Self-Attention:\n",
    "     * Query, Key, Value matrices: 3 * (embed_size * embed_size) = 3 * (64 * 64) = 12,288\n",
    "     * Output projection: embed_size * embed_size = 64 * 64 = 4,096\n",
    "   - Feed-forward network:\n",
    "     * First linear layer: embed_size * ff_dim = 64 * 128 = 8,192\n",
    "     * Second linear layer: ff_dim * embed_size = 128 * 64 = 8,192\n",
    "   - Layer Norm (2 per layer): 2 * 2 * embed_size = 2 * 2 * 64 = 256\n",
    "\n",
    "   Total per layer: 12,288 + 4,096 + 8,192 + 8,192 + 256 = 33,024\n",
    "\n",
    "3. Output Layer:\n",
    "   - Linear projection: embed_size * vocab_size = 64 * 14 = 896\n",
    "\n",
    "Now, let's sum it up:\n",
    "- Embedding Layer: 896 + 1,280 = 2,176\n",
    "- Transformer Layers: 33,024 * 2 = 66,048\n",
    "- Output Layer: 896\n",
    "\n",
    "Total parameters: 2,176 + 66,048 + 896 = 69,120\n",
    "\n",
    "So, this small transformer model would have approximately 69,120 parameters.\n",
    "\n",
    "This is a very small model, which is perfect for initial experiments and debugging on a CPU. It's about 3 orders of magnitude smaller than the models described in the Abacus Embeddings paper (which mentions models with ~12 million parameters), allowing for quick iterations and tests of the basic architecture and training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, max_length, num_samples):\n",
    "        # Initialize the dataset with maximum length of numbers and total samples\n",
    "        self.max_length = max_length\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "        # Define the vocabulary for tokenization\n",
    "        # 0-9 for digits, 10 for '+', 11 for '=', 12 for padding, 13 for end of sequence\n",
    "        self.vocab = {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, \n",
    "                      '+': 10, '=': 11, '<PAD>': 12, '<EOS>': 13}\n",
    "        # Create an inverse vocabulary for decoding\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        # Generate the dataset\n",
    "        self.data = self.generate_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a specific item from the dataset\n",
    "        return self.data[idx]\n",
    "\n",
    "    def generate_number(self, length):\n",
    "        # Generate a random number of specified length\n",
    "        return random.randint(10**(length-1), 10**length - 1)\n",
    "\n",
    "    def tokenize(self, s):\n",
    "        # Convert a string to a list of token IDs\n",
    "        return [self.vocab[c] for c in s if c in self.vocab]\n",
    "\n",
    "    def pad_sequence(self, seq, max_length):\n",
    "        # Pad a sequence with <PAD> tokens to reach the specified length\n",
    "        return seq + [self.vocab['<PAD>']] * (max_length - len(seq))\n",
    "\n",
    "    def generate_data(self):\n",
    "        data = []\n",
    "        # Calculate samples per length combination to achieve desired total samples\n",
    "        samples_per_combination = max(1, self.num_samples // (self.max_length ** 2))\n",
    "        \n",
    "        # Generate addition problems for all possible length combinations\n",
    "        for i in range(1, self.max_length + 1):\n",
    "            for j in range(1, self.max_length + 1):\n",
    "                for _ in range(samples_per_combination):\n",
    "                    # Generate two random numbers\n",
    "                    num1 = self.generate_number(i)\n",
    "                    num2 = self.generate_number(j)\n",
    "                    result = num1 + num2\n",
    "                    \n",
    "                    # Create the input string (reversed for right-to-left processing)\n",
    "                    input_str = f\"{num1:0{i}}+{num2:0{j}}=\"\n",
    "                    input_str = input_str[::-1]  # Reverse the string\n",
    "                    \n",
    "                    # Create the target string (reversed)\n",
    "                    target_str = f\"{result}\"[::-1]\n",
    "                    \n",
    "                    # Tokenize and pad both input and target\n",
    "                    input_tokens = self.tokenize(input_str)\n",
    "                    target_tokens = self.tokenize(target_str) + [self.vocab['<EOS>']]\n",
    "                    \n",
    "                    max_seq_length = self.max_length * 2 + 2  # Maximum possible sequence length\n",
    "                    input_padded = self.pad_sequence(input_tokens, max_seq_length)\n",
    "                    target_padded = self.pad_sequence(target_tokens, max_seq_length)\n",
    "                    \n",
    "                    # Convert to PyTorch tensors\n",
    "                    input_tensor = torch.tensor(input_padded, dtype=torch.long)\n",
    "                    target_tensor = torch.tensor(target_padded, dtype=torch.long)\n",
    "                    \n",
    "                    data.append((input_tensor, target_tensor))\n",
    "        \n",
    "        # Shuffle the data for randomness\n",
    "        random.shuffle(data)\n",
    "        return data\n",
    "\n",
    "    def decode(self, tensor):\n",
    "        # Convert a tensor of token IDs back to a string, reversing and removing special tokens\n",
    "        return ''.join(self.inv_vocab[t.item()] for t in tensor if t.item() not in [self.vocab['<PAD>'], self.vocab['<EOS>']])[::-1]\n",
    "\n",
    "# Set parameters for the dataset\n",
    "max_length = 20  # maximum length of operands\n",
    "train_samples = 200_000  # Number of training samples\n",
    "test_samples = 1_000  # Number of test samples\n",
    "\n",
    "# Create training and test datasets\n",
    "addition_train = AdditionDataset(max_length, train_samples)\n",
    "addition_test = AdditionDataset(max_length, test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition samples:\n",
      "Input: 9+203=\n",
      "Target: 212\n",
      "Equation: 9+203= 212\n",
      "\n",
      "Input: 547600+82709939191192837=\n",
      "Target: 82709939191740437\n",
      "Equation: 547600+82709939191192837= 82709939191740437\n",
      "\n",
      "Input: 754012+3=\n",
      "Target: 754015\n",
      "Equation: 754012+3= 754015\n",
      "\n",
      "Input: 4794181742+9611879=\n",
      "Target: 4803793621\n",
      "Equation: 4794181742+9611879= 4803793621\n",
      "\n",
      "Input: 6226+31918523272548940491=\n",
      "Target: 31918523272548946717\n",
      "Equation: 6226+31918523272548940491= 31918523272548946717\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print some samples\n",
    "print(\"Addition samples:\")\n",
    "for i in range(0,5):\n",
    "    input_tensor, target_tensor = addition_train[i]\n",
    "    input_str = addition_train.decode(input_tensor)\n",
    "    target_str = addition_train.decode(target_tensor)\n",
    "    print(f\"Input: {input_str}\")\n",
    "    print(f\"Target: {target_str}\")\n",
    "    print(f\"Equation: {input_str} {target_str}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbacusEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, max_length):\n",
    "        super().__init__()\n",
    "        # Create an embedding layer for the input tokens\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        # Create a separate embedding layer for positional encodings\n",
    "        self.pos_embed = nn.Embedding(max_length, embed_size)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get the sequence length of the input\n",
    "        seq_length = x.size(1)\n",
    "        \n",
    "        # Generate position indices\n",
    "        pos = torch.arange(seq_length, device=x.device).unsqueeze(0)\n",
    "        \n",
    "        # Truncate positions to max_length\n",
    "        # This ensures that positions beyond max_length use the same embedding\n",
    "        pos = torch.clamp(pos, max=self.max_length - 1)\n",
    "        \n",
    "        # Get the token embeddings\n",
    "        embedded = self.embed(x)\n",
    "        \n",
    "        # Get the positional embeddings\n",
    "        positional = self.pos_embed(pos)\n",
    "        \n",
    "        # Combine token embeddings and positional embeddings\n",
    "        return embedded + positional[:, :seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, ff_dim, num_layers, max_length):\n",
    "        super().__init__()\n",
    "        # Initialize the custom Abacus Embedding layer\n",
    "        self.embedding = AbacusEmbedding(vocab_size, embed_size, max_length)\n",
    "        \n",
    "        # Create a single Transformer encoder layer\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Create the full Transformer encoder by stacking multiple layers\n",
    "        self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final linear layer to project to vocabulary size\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            # Apply Abacus Embedding\n",
    "            x = self.embedding(x)\n",
    "            \n",
    "            # Pass through the Transformer encoder\n",
    "            x = self.transformer(x)\n",
    "            \n",
    "            # Project to vocabulary size\n",
    "            return self.fc_out(x)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in SmallTransformer forward pass: {str(e)}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs):\n",
    "    best_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create a progress bar for each epoch\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in progress_bar:\n",
    "            try:\n",
    "                # Reset gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update weights\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy for this batch\n",
    "                _, predicted = outputs.max(dim=-1)\n",
    "                non_pad_mask = targets.ne(addition_train.vocab['<PAD>'])\n",
    "                correct_predictions += (predicted[non_pad_mask] == targets[non_pad_mask]).sum().item()\n",
    "                total_predictions += non_pad_mask.sum().item()\n",
    "\n",
    "                # Update progress bar with current loss and accuracy\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f\"{loss.item():.4f}\",\n",
    "                    'acc': f\"{correct_predictions/total_predictions:.4f}\"\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                # Error handling and debugging information\n",
    "                print(f\"\\nError in batch {batch_idx}\")\n",
    "                print(f\"Input shape: {inputs.shape}, max value: {inputs.max().item()}, min value: {inputs.min().item()}\")\n",
    "                print(f\"Target shape: {targets.shape}, max value: {targets.max().item()}, min value: {targets.min().item()}\")\n",
    "                print(f\"Output shape: {outputs.shape}\")\n",
    "                raise e\n",
    "        \n",
    "        # Calculate average loss and accuracy for the epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = correct_predictions / total_predictions\n",
    "        epoch_time = time.time() - start_time\n",
    "\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs} - Time: {epoch_time:.2f}s')\n",
    "        print(f'Train Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "        \n",
    "        # Evaluation on test set\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = outputs.max(dim=-1)\n",
    "                non_pad_mask = targets.ne(addition_train.vocab['<PAD>'])\n",
    "                total += non_pad_mask.sum().item()\n",
    "                correct += (predicted[non_pad_mask] == targets[non_pad_mask]).sum().item()\n",
    "                \n",
    "                # Calculate test loss\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        # Calculate test accuracy and average test loss\n",
    "        test_accuracy = correct / total\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "        # Save the best model\n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'New best model saved with accuracy: {best_accuracy:.4f}')\n",
    "\n",
    "        print('-' * 60)\n",
    "\n",
    "    print(f'Training completed. Best test accuracy: {best_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/6250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6250/6250 [03:46<00:00, 27.65it/s, loss=1.3243, acc=0.3827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 - Time: 226.04s\n",
      "Train Loss: 1.6537, Train Accuracy: 0.3827\n",
      "Test Loss: 1.4574, Test Accuracy: 0.4344\n",
      "New best model saved with accuracy: 0.4344\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6250/6250 [03:54<00:00, 26.67it/s, loss=1.6127, acc=0.4225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/10 - Time: 234.32s\n",
      "Train Loss: 1.5003, Train Accuracy: 0.4225\n",
      "Test Loss: 1.4118, Test Accuracy: 0.4449\n",
      "New best model saved with accuracy: 0.4449\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6250/6250 [03:45<00:00, 27.69it/s, loss=1.3877, acc=0.4366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/10 - Time: 225.70s\n",
      "Train Loss: 1.4610, Train Accuracy: 0.4366\n",
      "Test Loss: 1.3644, Test Accuracy: 0.4725\n",
      "New best model saved with accuracy: 0.4725\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6250/6250 [03:49<00:00, 27.28it/s, loss=1.4293, acc=0.4684]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/10 - Time: 229.11s\n",
      "Train Loss: 1.3960, Train Accuracy: 0.4684\n",
      "Test Loss: 1.2328, Test Accuracy: 0.5350\n",
      "New best model saved with accuracy: 0.5350\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6250/6250 [18:04<00:00,  5.76it/s, loss=1.2539, acc=0.5084]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/10 - Time: 1084.67s\n",
      "Train Loss: 1.3149, Train Accuracy: 0.5084\n",
      "Test Loss: 1.1163, Test Accuracy: 0.5858\n",
      "New best model saved with accuracy: 0.5858\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6250/6250 [03:58<00:00, 26.24it/s, loss=1.1787, acc=0.5408]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/10 - Time: 238.23s\n",
      "Train Loss: 1.2441, Train Accuracy: 0.5408\n",
      "Test Loss: 1.0412, Test Accuracy: 0.6149\n",
      "New best model saved with accuracy: 0.6149\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6250/6250 [03:26<00:00, 30.26it/s, loss=1.3563, acc=0.5738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/10 - Time: 206.56s\n",
      "Train Loss: 1.1618, Train Accuracy: 0.5738\n",
      "Test Loss: 0.9037, Test Accuracy: 0.6607\n",
      "New best model saved with accuracy: 0.6607\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6250/6250 [03:28<00:00, 29.99it/s, loss=1.3578, acc=0.5975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/10 - Time: 208.38s\n",
      "Train Loss: 1.0946, Train Accuracy: 0.5975\n",
      "Test Loss: 0.8801, Test Accuracy: 0.6699\n",
      "New best model saved with accuracy: 0.6699\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6250/6250 [03:45<00:00, 27.66it/s, loss=0.9276, acc=0.6073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/10 - Time: 225.94s\n",
      "Train Loss: 1.0679, Train Accuracy: 0.6073\n",
      "Test Loss: 0.8410, Test Accuracy: 0.6808\n",
      "New best model saved with accuracy: 0.6808\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6250/6250 [03:00<00:00, 34.56it/s, loss=0.9931, acc=0.6154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/10 - Time: 180.84s\n",
      "Train Loss: 1.0465, Train Accuracy: 0.6154\n",
      "Test Loss: 0.8201, Test Accuracy: 0.6943\n",
      "New best model saved with accuracy: 0.6943\n",
      "------------------------------------------------------------\n",
      "Training completed. Best test accuracy: 0.6943\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "vocab_size = 14  # 0-9 digits <PAD>, <EOS>, +, =,\n",
    "embed_size = 64\n",
    "num_heads = 2\n",
    "ff_dim = 128\n",
    "num_layers = 2\n",
    "max_length = 20\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(addition_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(addition_test, batch_size=batch_size)\n",
    "\n",
    "max_seq_length = max_length * 2 + 2  # This should be 42 based on your current setup\n",
    "model = SmallTransformer(vocab_size, embed_size, num_heads, ff_dim, num_layers, max_seq_length)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_size-2)  # Assuming <PAD> is the second to last token\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# After training is complete\u001b[39;00m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab_size\u001b[39m\u001b[38;5;124m'\u001b[39m: vocab_size,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membed_size\u001b[39m\u001b[38;5;124m'\u001b[39m: embed_size,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_heads\u001b[39m\u001b[38;5;124m'\u001b[39m: num_heads,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: ff_dim,\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: num_layers,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_seq_length\u001b[39m\u001b[38;5;124m'\u001b[39m: max_seq_length\n\u001b[1;32m     11\u001b[0m }, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_addition_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# After training is complete\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'embed_size': embed_size,\n",
    "    'num_heads': num_heads,\n",
    "    'ff_dim': ff_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'max_seq_length': max_seq_length\n",
    "}, 'trained_addition_model.pth')\n",
    "\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "checkpoint = torch.load('trained_addition_model.pth')\n",
    "\n",
    "# Recreate the model architecture\n",
    "loaded_model = SmallTransformer(\n",
    "    checkpoint['vocab_size'],\n",
    "    checkpoint['embed_size'],\n",
    "    checkpoint['num_heads'],\n",
    "    checkpoint['ff_dim'],\n",
    "    checkpoint['num_layers'],\n",
    "    checkpoint['max_seq_length']\n",
    ")\n",
    "\n",
    "# Load the model weights\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "loaded_model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Function to preprocess input for the model\n",
    "def preprocess_input(input_str, max_length):\n",
    "    # Reverse the input string\n",
    "    input_str = input_str[::-1]\n",
    "    # Tokenize\n",
    "    tokens = [addition_train.vocab[c] for c in input_str if c in addition_train.vocab]\n",
    "    # Pad\n",
    "    padded = tokens + [addition_train.vocab['<PAD>']] * (max_length - len(tokens))\n",
    "    return torch.tensor(padded).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Function to decode model output\n",
    "def decode_output(output_tensor):\n",
    "    _, predicted = output_tensor.max(2)\n",
    "    decoded = ''.join([addition_train.inv_vocab[t.item()] for t in predicted[0] if t.item() not in [addition_train.vocab['<PAD>'], addition_train.vocab['<EOS>']]])\n",
    "    return decoded[::-1]  # Reverse the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1234 + 7890 = 9024\n",
      "Correct result: 9124\n",
      "Model's prediction is incorrect\n"
     ]
    }
   ],
   "source": [
    "# Test the model on a single addition problem\n",
    "def test_addition(num1, num2):\n",
    "    input_str = f\"{num1}+{num2}=\"\n",
    "    input_tensor = preprocess_input(input_str, checkpoint['max_seq_length'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = loaded_model(input_tensor)\n",
    "    \n",
    "    result = decode_output(output)\n",
    "    print(f\"{num1} + {num2} = {result}\")\n",
    "    print(f\"Correct result: {num1 + num2}\")\n",
    "    print(f\"Model's prediction is {'correct' if int(result) == num1 + num2 else 'incorrect'}\")\n",
    "\n",
    "# Test on some examples\n",
    "#test_addition(123, 456)\n",
    "test_addition(1234,7890)\n",
    "#test_addition(4, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_test_set(num_samples, max_digits):\n",
    "    test_set = []\n",
    "    for _ in range(num_samples):\n",
    "        num1 = random.randint(1, 10**max_digits - 1)\n",
    "        num2 = random.randint(1, 10**max_digits - 1)\n",
    "        result = num1 + num2\n",
    "        test_set.append((num1, num2, result))\n",
    "    return test_set\n",
    "\n",
    "# Generate a test set\n",
    "num_test_samples = 1000\n",
    "max_test_digits = 20  # Maximum number of digits in each operand\n",
    "test_set = generate_test_set(num_test_samples, max_test_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Accuracy: 0.6875\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m train_accuracy \u001b[38;5;241m=\u001b[39m evaluate_on_dataset(loaded_model, train_loader_for_eval, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluate on test data for comparison\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m evaluate_on_dataset(loaded_model, \u001b[43mtest_loader\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Print comparison\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAccuracy comparison:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "def evaluate_on_dataset(model, dataloader, dataset_name=\"Dataset\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(2)\n",
    "            \n",
    "            # Create a mask for non-padding tokens\n",
    "            non_pad_mask = targets.ne(addition_train.vocab['<PAD>'])\n",
    "            \n",
    "            # Count correct predictions\n",
    "            correct += (predicted[non_pad_mask] == targets[non_pad_mask]).sum().item()\n",
    "            total += non_pad_mask.sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"{dataset_name} Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "# Create a DataLoader for the training data\n",
    "train_loader_for_eval = DataLoader(addition_train, batch_size=32, shuffle=False)\n",
    "\n",
    "# Evaluate on training data\n",
    "train_accuracy = evaluate_on_dataset(loaded_model, train_loader_for_eval, \"Training Data\")\n",
    "\n",
    "# Evaluate on test data for comparison\n",
    "test_accuracy = evaluate_on_dataset(loaded_model, test_loader, \"Test Data\")\n",
    "\n",
    "# Print comparison\n",
    "print(f\"\\nAccuracy comparison:\")\n",
    "print(f\"Training Data: {train_accuracy:.4f}\")\n",
    "print(f\"Test Data:     {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 100 large number samples (21-30 digits): 0.0000\n",
      "Testing on specific addition patterns:\n",
      "999999 + 1 = 99991999900 (True: 1000000)\n",
      "Correct: False\n",
      "\n",
      "1 + 999999 = 11999000 (True: 1000000)\n",
      "Correct: False\n",
      "\n",
      "999999999999999 + 1 = 00999990099999990999999999999999900 (True: 1000000000000000)\n",
      "Correct: False\n",
      "\n",
      "1000000000000000 + 1000000000000000 = 0000011010002000000000000000 (True: 2000000000000000)\n",
      "Correct: False\n",
      "\n",
      "123456789 + 987654321 = 91951616571110877600 (True: 1111111110)\n",
      "Correct: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_larger_additions(model, max_seq_length, num_samples=100):\n",
    "    correct = 0\n",
    "    for _ in range(num_samples):\n",
    "        num1 = random.randint(10**20, 10**30 - 1)  # 21 to 30 digit numbers\n",
    "        num2 = random.randint(10**20, 10**30 - 1)\n",
    "        true_result = num1 + num2\n",
    "        \n",
    "        input_str = f\"{num1}+{num2}=\"\n",
    "        input_tensor = preprocess_input(input_str, max_seq_length)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        \n",
    "        predicted_result = decode_output(output)\n",
    "        \n",
    "        try:\n",
    "            if int(predicted_result) == true_result:\n",
    "                correct += 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    accuracy = correct / num_samples\n",
    "    print(f\"Accuracy on {num_samples} large number samples (21-30 digits): {accuracy:.4f}\")\n",
    "\n",
    "# Test on larger numbers\n",
    "test_larger_additions(loaded_model, checkpoint['max_seq_length'])\n",
    "\n",
    "def test_specific_patterns(model, max_seq_length):\n",
    "    test_cases = [\n",
    "        (999999, 1),  # Testing carry over\n",
    "        (1, 999999),  # Testing different order\n",
    "        (10**15 - 1, 1),  # Large number + small number\n",
    "        (10**15, 10**15),  # Two large, round numbers\n",
    "        (123456789, 987654321),  # Ascending + descending\n",
    "    ]\n",
    "    \n",
    "    for num1, num2 in test_cases:\n",
    "        input_str = f\"{num1}+{num2}=\"\n",
    "        input_tensor = preprocess_input(input_str, max_seq_length)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "        \n",
    "        predicted_result = decode_output(output)\n",
    "        true_result = num1 + num2\n",
    "        \n",
    "        print(f\"{num1} + {num2} = {predicted_result} (True: {true_result})\")\n",
    "        print(f\"Correct: {int(predicted_result) == true_result}\")\n",
    "        print()\n",
    "\n",
    "# Test on specific patterns\n",
    "print(\"Testing on specific addition patterns:\")\n",
    "test_specific_patterns(loaded_model, checkpoint['max_seq_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchinfo  # Better alternative to torchsummary for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================================================================================================\n",
      "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
      "==========================================================================================================================================================================\n",
      "SmallTransformer                              [32, 42]                  [32, 42, 14]              33,472                    --                        --\n",
      "├─AbacusEmbedding: 1-1                        [32, 42]                  [32, 42, 64]              --                        --                        --\n",
      "│    └─Embedding: 2-1                         [32, 42]                  [32, 42, 64]              896                       --                        28,672\n",
      "│    └─Embedding: 2-2                         [1, 42]                   [1, 42, 64]               2,688                     --                        2,688\n",
      "├─TransformerEncoder: 1-2                     [32, 42, 64]              [32, 42, 64]              --                        --                        --\n",
      "│    └─ModuleList: 2-3                        --                        --                        --                        --                        --\n",
      "│    │    └─TransformerEncoderLayer: 3-1      [32, 42, 64]              [32, 42, 64]              33,472                    --                        --\n",
      "│    │    └─TransformerEncoderLayer: 3-2      [32, 42, 64]              [32, 42, 64]              33,472                    --                        --\n",
      "├─Linear: 1-3                                 [32, 42, 64]              [32, 42, 14]              910                       --                        29,120\n",
      "==========================================================================================================================================================================\n",
      "Total params: 104,910\n",
      "Trainable params: 104,910\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.06\n",
      "==========================================================================================================================================================================\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.86\n",
      "Params size (MB): 0.02\n",
      "Estimated Total Size (MB): 0.89\n",
      "==========================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import torchinfo  # Better alternative to torchsummary for transformers\n",
    "\n",
    "def summarize_transformer(model, batch_size=32, seq_length=42, vocab_size=None):\n",
    "    \"\"\"\n",
    "    Provides a detailed summary of a transformer model using torchinfo.\n",
    "    \n",
    "    Args:\n",
    "        model: The transformer model to analyze\n",
    "        batch_size: Number of samples in a batch\n",
    "        seq_length: Length of input sequences\n",
    "        vocab_size: Size of vocabulary (if None, will use model's vocab_size if available)\n",
    "    \"\"\"\n",
    "    # Create dummy input tensor with proper dtype\n",
    "    if vocab_size is None:\n",
    "        try:\n",
    "            vocab_size = model.vocab_size\n",
    "        except AttributeError:\n",
    "            vocab_size = 1000  # default fallback\n",
    "    \n",
    "    # Generate random indices within vocab size range\n",
    "    dummy_input = torch.randint(0, vocab_size, (batch_size, seq_length), dtype=torch.long)\n",
    "    \n",
    "    # Get model summary\n",
    "    summary = torchinfo.summary(\n",
    "        model,\n",
    "        input_data=dummy_input,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"],\n",
    "        depth=4,  # Adjust this to see more/less layers\n",
    "        device='cpu'  # Change to 'cuda' if using GPU\n",
    "    )\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "    # Get model summary\n",
    "summary = summarize_transformer(\n",
    "    loaded_model,\n",
    "    batch_size=32,\n",
    "    seq_length=42,\n",
    "    vocab_size=checkpoint['vocab_size']\n",
    "    )\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Transformer Model for Arithmetic Operations\n",
    "\n",
    "This is a compact transformer model designed for arithmetic tasks. Here's a breakdown of its architecture:\n",
    "\n",
    "## Model Overview\n",
    "- **Total Parameters**: 104,910 (very lightweight!)\n",
    "- **Memory Footprint**: Only 0.89 MB\n",
    "- **Max Sequence Length**: 42 tokens\n",
    "- **Embedding Dimension**: 64\n",
    "- **Vocabulary Size**: 14 (likely digits 0-9 plus special tokens)\n",
    "\n",
    "## Architecture Components\n",
    "\n",
    "### 1. Input Processing (AbacusEmbedding)\n",
    "- **Token Embedding**: Converts each input number/symbol into a 64-dimensional vector\n",
    "- **Positional Embedding**: Adds position information to each token to maintain sequence order\n",
    "- These embeddings combine to give the model understanding of both WHAT each token is and WHERE it appears\n",
    "\n",
    "### 2. Transformer Encoder\n",
    "- **Number of Layers**: 2\n",
    "- Each layer contains:\n",
    " - Self-attention mechanism (allows model to weigh importance of different positions)\n",
    " - Feed-forward neural network\n",
    "- Helps model understand relationships between different positions in the input sequence\n",
    "\n",
    "### 3. Output Layer\n",
    "- Linear projection layer that converts the 64-dimensional features back to vocabulary size (14)\n",
    "- Produces predictions for each position in the sequence\n",
    "\n",
    "This model's architecture suggests it's optimized for tasks like addition or basic arithmetic, where it needs to process sequences of numbers and operators. The small vocabulary size (14) is perfect for digits 0-9 plus a few special tokens (like '+', '=', etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
